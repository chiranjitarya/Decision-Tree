{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "Ans:- Decision Tree Overview\n",
        "\n",
        "A Decision Tree is a type of supervised learning algorithm used for both classification and regression tasks. It's a tree-like model that splits data into subsets based on features or attributes.\n",
        "\n",
        "How Decision Trees Work in Classification\n",
        "\n",
        "In the context of classification, a Decision Tree works as follows:\n",
        "\n",
        "1. Root Node: The algorithm starts with a root node representing the entire dataset.\n",
        "2. Splitting: The algorithm selects the best feature to split the data into subsets based on a specific criterion (e.g., Gini impurity or entropy).\n",
        "3. Decision Nodes: Each internal node represents a decision or split based on a feature.\n",
        "4. Leaf Nodes: Each leaf node represents a class label or prediction.\n",
        "5. Recursion: The algorithm recursively splits the data until a stopping criterion is met (e.g., all instances belong to the same class or a maximum depth is reached).\n",
        "\n",
        "Decision Tree Classification Example\n",
        "\n",
        "Suppose we want to classify animals into \"mammal\" or \"non-mammal\" based on features like \"has fur\" and \"produces milk.\" A Decision Tree might look like this:\n",
        "\n",
        "- Root Node: All animals\n",
        "- Decision Node 1: Has fur?\n",
        "    - Yes: Produces milk? (Decision Node 2)\n",
        "        - Yes: Mammal (Leaf Node)\n",
        "        - No: Non-mammal (Leaf Node)\n",
        "    - No: Non-mammal (Leaf Node)\n",
        "\n",
        "Advantages and Limitations\n",
        "\n",
        "Decision Trees are intuitive, easy to interpret, and can handle both categorical and numerical features. However, they can suffer from overfitting, especially when dealing with complex datasets.\n",
        "\n",
        "Real-World Applications\n",
        "\n",
        "Decision Trees are widely used in various applications, such as:\n",
        "\n",
        "- Credit risk assessment\n",
        "- Medical diagnosis\n",
        "- Customer segmentation\n",
        "- Image classification\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "Ans:- Gini Impurity and Entropy: Impurity Measures in Decision Trees\n",
        "\n",
        "Gini Impurity and Entropy are two commonly used impurity measures in Decision Trees. They help determine the best feature to split the data at each node.\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Gini Impurity measures the probability of misclassifying a randomly chosen instance from a node. It's calculated as:\n",
        "\n",
        "Gini Impurity = 1 - Σ (probability of each class)^2\n",
        "\n",
        "where the probability is calculated based on the class distribution in the node.\n",
        "\n",
        "Entropy\n",
        "\n",
        "Entropy measures the uncertainty or randomness in the class distribution of a node. It's calculated as:\n",
        "\n",
        "Entropy = - Σ (probability of each class) * log2(probability of each class)\n",
        "\n",
        "Impact on Splits in Decision Trees\n",
        "\n",
        "Both Gini Impurity and Entropy are used to evaluate the quality of splits in a Decision Tree. The goal is to find the feature that results in the largest reduction in impurity or uncertainty.\n",
        "\n",
        "When a node is split, the algorithm calculates the weighted average of the impurity measures for the child nodes. The feature that results in the largest reduction in impurity is chosen for the split.\n",
        "\n",
        "Comparison of Gini Impurity and Entropy\n",
        "\n",
        "Both Gini Impurity and Entropy are effective impurity measures, but they have some differences:\n",
        "\n",
        "- Gini Impurity is more sensitive to changes in class probabilities, while Entropy is more sensitive to changes in the number of classes.\n",
        "- Gini Impurity is generally faster to compute than Entropy.\n",
        "\n",
        "In practice, both measures often produce similar results, and the choice between them usually depends on the specific problem or personal preference.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we have a node with 10 instances, 6 of which belong to class A and 4 of which belong to class B. The Gini Impurity would be:\n",
        "\n",
        "Gini Impurity = 1 - (6/10)^2 - (4/10)^2 = 0.48\n",
        "\n",
        "The Entropy would be:\n",
        "\n",
        "Entropy = - (6/10) * log2(6/10) - (4/10) * log2(4/10) = 0.97\n",
        "\n",
        "If we split the node based on a feature, we'd calculate the weighted average of the impurity measures for the child nodes and choose the feature that results in the largest reduction in impurity.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "Ans:- Pre-Pruning vs Post-Pruning in Decision Trees\n",
        "\n",
        "Pre-Pruning and Post-Pruning are two techniques used to prevent overfitting in Decision Trees.\n",
        "\n",
        "Pre-Pruning (Early Stopping)\n",
        "\n",
        "Pre-Pruning involves stopping the growth of the Decision Tree before it reaches its maximum depth or captures all the noise in the training data. This is typically done by specifying a set of hyperparameters, such as:\n",
        "\n",
        "- Maximum depth\n",
        "- Minimum number of samples required to split a node\n",
        "- Minimum number of samples required at a leaf node\n",
        "\n",
        "Post-Pruning (Reduced Error Pruning)\n",
        "\n",
        "Post-Pruning involves growing the Decision Tree to its full depth and then removing branches that do not contribute significantly to the model's performance. This is typically done by:\n",
        "\n",
        "- Calculating the error rate for each subtree\n",
        "- Removing branches that do not reduce the error rate\n",
        "\n",
        "Practical Advantages\n",
        "\n",
        "- Pre-Pruning Advantage: One practical advantage of Pre-Pruning is that it reduces computational cost. By stopping the growth of the tree early, you can avoid unnecessary computations and reduce the risk of overfitting.\n",
        "- Post-Pruning Advantage: One practical advantage of Post-Pruning is that it allows for more flexibility. By growing the tree to its full depth, you can capture complex interactions between features and then prune the tree to remove unnecessary branches.\n",
        "\n",
        "Choosing Between Pre-Pruning and Post-Pruning\n",
        "\n",
        "The choice between Pre-Pruning and Post-Pruning depends on the specific problem and dataset. Pre-Pruning can be useful when working with large datasets or when computational resources are limited. Post-Pruning can be useful when you want to capture complex interactions between features and then simplify the model.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "Ans:- Information Gain in Decision Trees\n",
        "\n",
        "Information Gain is a measure used to evaluate the quality of a split in a Decision Tree. It calculates the reduction in uncertainty or entropy after splitting the data based on a particular feature.\n",
        "\n",
        "How Information Gain Works\n",
        "\n",
        "Information Gain is calculated as the difference between the entropy of the parent node and the weighted average of the entropy of the child nodes.\n",
        "\n",
        "Information Gain = Entropy (Parent) - Σ (|Child Node| / |Parent Node|) * Entropy (Child Node)\n",
        "\n",
        "where |Child Node| and |Parent Node| represent the number of instances in the child and parent nodes, respectively.\n",
        "\n",
        "Importance of Information Gain\n",
        "\n",
        "Information Gain is important for choosing the best split in a Decision Tree because it helps to:\n",
        "\n",
        "- Reduce uncertainty: By maximizing Information Gain, the Decision Tree algorithm can reduce the uncertainty or entropy in the data, resulting in more accurate predictions.\n",
        "- Identify relevant features: Information Gain helps to identify the most relevant features for splitting the data, which can improve the model's performance and interpretability.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we have a Decision Tree node with an entropy of 1.0, and we split it based on a feature that results in two child nodes with entropies of 0.5 and 0.8, respectively. If the child nodes have 60% and 40% of the instances, the Information Gain would be:\n",
        "\n",
        "Information Gain = 1.0 - (0.6 * 0.5 + 0.4 * 0.8) = 0.38\n",
        "\n",
        "The feature that results in the highest Information Gain is chosen for the split.\n",
        "\n",
        "Why Information Gain Matters\n",
        "\n",
        "Information Gain is a key concept in Decision Trees because it helps to:\n",
        "\n",
        "- Improve model performance: By choosing the best splits based on Information Gain, Decision Trees can improve their accuracy and robustness.\n",
        "- Simplify the model: By selecting the most relevant features, Information Gain can help to simplify the model and reduce the risk of overfitting.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "Ans:- Real-World Applications of Decision Trees\n",
        "\n",
        "Decision Trees have numerous applications across various industries, including:\n",
        "\n",
        "1. Credit Risk Assessment: Decision Trees are used to evaluate creditworthiness and predict the likelihood of loan defaults.\n",
        "2. Medical Diagnosis: Decision Trees are used to diagnose diseases and predict patient outcomes based on symptoms, medical history, and test results.\n",
        "3. Customer Segmentation: Decision Trees are used to segment customers based on demographic, behavioral, and transactional data.\n",
        "4. Image Classification: Decision Trees are used in image classification tasks, such as object detection and recognition.\n",
        "5. Business Decision-Making: Decision Trees are used to support business decisions, such as predicting customer churn, identifying market trends, and optimizing marketing campaigns.\n",
        "\n",
        "Advantages of Decision Trees\n",
        "\n",
        "1. Interpretability: Decision Trees are easy to interpret and understand, making them a popular choice for many applications.\n",
        "2. Flexibility: Decision Trees can handle both categorical and numerical features, and can be used for classification and regression tasks.\n",
        "3. Handling Missing Values: Decision Trees can handle missing values in the data, making them robust to incomplete data.\n",
        "4. Fast Training: Decision Trees are relatively fast to train, especially compared to more complex models.\n",
        "\n",
        "Limitations of Decision Trees\n",
        "\n",
        "1. Overfitting: Decision Trees can suffer from overfitting, especially when the trees are deep or complex.\n",
        "2. Instability: Decision Trees can be unstable, meaning that small changes in the data can result in large changes in the tree structure.\n",
        "3. Greedy Algorithm: Decision Trees use a greedy algorithm, which can lead to suboptimal splits and reduced performance.\n",
        "4. Limited Handling of Complex Relationships: Decision Trees can struggle to capture complex relationships between features, which can limit their performance in certain applications.\n",
        "\n",
        "Overcoming Limitations\n",
        "\n",
        "To overcome the limitations of Decision Trees, techniques such as:\n",
        "\n",
        "1. Ensemble Methods: Combining multiple Decision Trees can improve performance and reduce overfitting.\n",
        "2. Pruning: Pruning Decision Trees can help to reduce overfitting and improve generalization.\n",
        "3. Feature Engineering: Careful feature engineering can help to capture complex relationships between features and improve Decision Tree performance.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "zhOItTRgbILN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dseSairQ3Twr",
        "outputId": "b5e95ca9-d2b5-4949-d0c3-c0483c9ffb6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.00\n",
            "sepal width (cm): 0.02\n",
            "petal length (cm): 0.91\n",
            "petal width (cm): 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "Wpta44s23sEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "clf_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth_3.fit(X_train, y_train)\n",
        "y_pred_depth_3 = clf_depth_3.predict(X_test)\n",
        "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_depth_3:.2f}\")\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.2f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "print(f\"Difference in accuracy: {accuracy_full - accuracy_depth_3:.2f}\")\n",
        "if accuracy_depth_3 > accuracy_full:\n",
        "    print(\"The tree with max_depth=3 performs better.\")\n",
        "elif accuracy_depth_3 < accuracy_full:\n",
        "    print(\"The fully-grown tree performs better.\")\n",
        "else:\n",
        "    print(\"Both trees perform equally well.\")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wr_Ffbh3t09",
        "outputId": "132538c7-5428-4ac1-9d40-e554f95ec08f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy of fully-grown tree: 1.00\n",
            "Difference in accuracy: 0.00\n",
            "Both trees perform equally well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "qgmkmYJm4X5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the California Housing dataset\n",
        "cal_housing = fetch_california_housing()\n",
        "X = cal_housing.data\n",
        "y = cal_housing.target\n",
        "feature_names = cal_housing.feature_names\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = regressor.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUJrfXbD4fxf",
        "outputId": "dd12e1d7-edfe-4272-bf9b-603eda2e0014"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "Feature Importances:\n",
            "MedInc: 0.53\n",
            "HouseAge: 0.05\n",
            "AveRooms: 0.05\n",
            "AveBedrms: 0.03\n",
            "Population: 0.03\n",
            "AveOccup: 0.13\n",
            "Latitude: 0.09\n",
            "Longitude: 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "l5TJQQzg5dpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHD-QqHR5i7_",
        "outputId": "7a01a085-132f-4550-b694-ae2793d1613c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Best Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting\n",
        "\n",
        "Ans:- Predicting Disease Presence using Decision Trees\n",
        "\n",
        "Here's a step-by-step process to handle missing values, encode categorical features, train a Decision Tree model, tune its hyperparameters, and evaluate its performance:\n",
        "\n",
        "Step 1: Handle Missing Values1. Identify missing values: Use pandas to identify missing values in the dataset.\n",
        "2. Determine the type of missing values: Determine whether the missing values are Missing At Random (MAR), Missing Completely At Random (MCAR), or Missing Not At Random (MNAR).\n",
        "3. Choose an imputation strategy: Based on the type of missing values, choose an imputation strategy such as mean/median imputation, imputation using regression, or multiple imputation.\n",
        "4. Impute missing values: Use the chosen imputation strategy to fill in the missing values.\n",
        "\n",
        "Step 2: Encode Categorical Features1. Identify categorical features: Identify the categorical features in the dataset.\n",
        "2. Choose an encoding strategy: Choose an encoding strategy such as one-hot encoding, label encoding, or ordinal encoding.\n",
        "3. Encode categorical features: Use the chosen encoding strategy to transform the categorical features into numerical features.\n",
        "\n",
        "Step 3: Train a Decision Tree Model1. Split the data: Split the dataset into training and testing sets.\n",
        "2. Train a Decision Tree model: Train a Decision Tree model on the training data using a suitable library such as scikit-learn.\n",
        "3. Make predictions: Use the trained model to make predictions on the testing data.\n",
        "\n",
        "Step 4: Tune Hyperparameters1. Define a hyperparameter grid: Define a grid of hyperparameters to tune, such as max_depth, min_samples_split, and min_samples_leaf.\n",
        "2. Perform grid search: Use a grid search algorithm such as GridSearchCV to find the best combination of hyperparameters.\n",
        "3. Evaluate the best model: Evaluate the performance of the best model on the testing data.\n",
        "\n",
        "Step 5: Evaluate Performance1. Choose evaluation metrics: Choose suitable evaluation metrics such as accuracy, precision, recall, F1-score, and ROC-AUC score.\n",
        "2. Calculate evaluation metrics: Calculate the evaluation metrics for the best model.\n",
        "3. Compare to baseline: Compare the performance of the best model to a baseline model or a competing model.\n",
        "\n",
        "Business ValueThe Decision Tree model can provide significant business value in the real-world setting by:\n",
        "\n",
        "1. Improving disease diagnosis: The model can help doctors diagnose diseases more accurately and quickly, leading to better patient outcomes.\n",
        "2. Reducing costs: The model can help reduce costs associated with unnecessary tests and procedures.\n",
        "3. Enhancing patient care: The model can help identify high-risk patients and enable early interventions, leading to better patient care.\n",
        "4. Informing treatment decisions: The model can provide insights into the most effective treatment options for patients, enabling personalized medicine.\n",
        "\n"
      ],
      "metadata": {
        "id": "PQ0P6-Dd52rJ"
      }
    }
  ]
}